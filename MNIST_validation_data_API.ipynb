{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-validation-data-API.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebaranas/colab/blob/master/MNIST_validation_data_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Roc56O-Czamw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, model_name, params):\n",
        "        self.NUM_CLASSES = params['NUM_CLASSES']\n",
        "        SUPPORTED_MODELS= {\"FullyConnected\": (self.FullyConnected, [2]),\n",
        "                    \"LowResFrameClassifier\": (self.LowResFrameClassifier, [4]),\n",
        "                    \"SimpleFullyConnected\":(self.SimpleFullyConnected, [2])\n",
        "        }\n",
        "        self.model_function, self.required_dims = SUPPORTED_MODELS.get(model_name, 'KeyError')\n",
        "        \n",
        "    def get_model_function(self):\n",
        "        return self.model_function\n",
        "    \n",
        "    def get_required_dims(self):\n",
        "        # For now first element of list is default.\n",
        "        return self.required_dims\n",
        "    \n",
        "    def train(self, next_batch):\n",
        "        logit, label = self.model_function(next_batch[0]), next_batch[1]\n",
        "        loss = self.get_loss(logit, label)\n",
        "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        accuracy = self.get_accuracy(logit, label)\n",
        "        return loss, optimizer, accuracy\n",
        "    \n",
        "    def get_loss(self, logit, label):\n",
        "        softmax = tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit)\n",
        "        return tf.reduce_sum(softmax)\n",
        "\n",
        "    def get_accuracy(self, logit, label):\n",
        "        prediction = tf.argmax(logit, 1)\n",
        "        equality = tf.equal(prediction, tf.argmax(label, 1))\n",
        "        return tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        \n",
        "    def FullyConnected(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        fc1 = tf.layers.dense(feature, 50)\n",
        "        fc2 = tf.layers.dense(fc1, 50)\n",
        "        fc2 = tf.layers.dropout(fc2)\n",
        "        flat = tf.layers.flatten(fc2) # added flatten layer 08/10/2018 to correct shape\n",
        "        fc3 = tf.layers.dense(flat, self.NUM_CLASSES)\n",
        "        return fc3\n",
        "        \n",
        "    def SimpleFullyConnected(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        fc1 = tf.layers.dense(feature, 30)\n",
        "        final = tf.layers.dense(fc1, self.NUM_CLASSES)\n",
        "        return final\n",
        "    \n",
        "    def LowResFrameClassifier(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        conv1 = tf.layers.conv2d(feature, 32, (3, 3), activation=\"relu\")\n",
        "        conv2 = tf.layers.conv2d(conv1, 32, (3, 3), activation=\"relu\")\n",
        "        maxpool2d_a = tf.layers.max_pooling2d(conv2, pool_size=(2, 2), strides=(1, 1))\n",
        "        dropout_a = tf.layers.dropout(maxpool2d_a, rate=0.25)\n",
        "        \n",
        "        conv3 = tf.layers.conv2d(dropout_a, 64, (3, 3), activation=\"relu\")\n",
        "        conv4 = tf.layers.conv2d(conv3, 64, (3, 3), activation=\"relu\")\n",
        "        maxpool2d_b = tf.layers.max_pooling2d(conv3, pool_size=(2, 2), strides=(1, 1))\n",
        "        dropout_b = tf.layers.dropout(maxpool2d_b, rate=0.25)\n",
        "        \n",
        "        flat = tf.layers.flatten(dropout_b)\n",
        "        dense = tf.layers.dense(flat, 256, activation=\"relu\")\n",
        "        dropout_c = tf.layers.dropout(dense, rate=0.25)\n",
        "        \n",
        "        final = tf.layers.dense(dropout_c, self.NUM_CLASSES, activation=\"softmax\")\n",
        "        return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JD-bdMcozqaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2fa828dc-50e8-449d-9f95-5dd7464f59a0"
      },
      "cell_type": "code",
      "source": [
        "!pip install python-mnist\n",
        "from mnist import MNIST"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/05/9c/f1c1e4d011b01ac436bba0ac6715b3f988bb7f8fec6f21f89cf820aa33e1/python-mnist-0.6.tar.gz\n",
            "Building wheels for collected packages: python-mnist\n",
            "  Running setup.py bdist_wheel for python-mnist ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/28/29/36/408f83545a511c43d03ef997a1dc99b49ccd5f9f306ed92468\n",
            "Successfully built python-mnist\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jd9ZqyjEzlsj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mnist import MNIST\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def read_raw_mnist(data_path):\n",
        "    mndata = MNIST(data_path)\n",
        "    images, labels = mndata.load_training()\n",
        "    return np.asarray(images), np.asarray(labels)\n",
        "\n",
        "def read_raw_bfimage(data_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label_index, label in sorted(enumerate(os.listdir(data_path))):\n",
        "        filename = data_path + label\n",
        "        label = label.replace(\".npy\", \"\")\n",
        "        for image in np.load(filename): # will load one npy file which may contain many examples\n",
        "            images.append(image)\n",
        "            labels.append(label_index)\n",
        "    return np.asarray(images), np.asarray(labels)\n",
        "\n",
        "SUPPORTED_DATA = {\"mnist\": read_raw_mnist, \"bfimage\": read_raw_bfimage}\n",
        "\n",
        "def read_raw(data_name, data_path):\n",
        "    read = (SUPPORTED_DATA.get(data_name, 'KeyError'))\n",
        "    return read(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M87oyOH7zop4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import toml\n",
        "from tensorflow.python.client import timeline\n",
        "from multiprocessing import Pool\n",
        "\n",
        "params = {'DATA_NAME':'mnist', 'DATA_PATH':'.', 'DTYPE':'np.float32', 'HEIGHT':28, \n",
        "          'WIDTH':28, 'CHANNELS':1, 'NUM_CLASSES':10, 'BATCH_SIZE':48, 'TRAIN_RATIO':0.8, 'NUM_CORES':4}\n",
        "\n",
        "class Benchmark(object):\n",
        "    def __init__(self, model_name):\n",
        "        print(\"Training '{model}' on '{data}'\".format(model=model_name,\n",
        "            data=params['DATA_NAME']))\n",
        "        raw_features, raw_labels = read_raw(params['DATA_NAME'], params['DATA_PATH'])\n",
        "        \n",
        "        params.update({'NUM_EXAMPLES': len(raw_features)})\n",
        "        params.update({'TRAIN_SIZE': int(params['TRAIN_RATIO']*params['NUM_EXAMPLES'])})\n",
        "        params.update({'VALID_SIZE': int(params['NUM_EXAMPLES'] - params['TRAIN_SIZE'])})\n",
        "        self.params = params\n",
        "        self.model = Model(model_name, params)\n",
        "        \n",
        "        reshape = self.should_reshape(rank=len(raw_features.shape))\n",
        "        dtype = eval(params['DTYPE'])\n",
        "        features, labels = self.preprocess(raw_features, raw_labels, dtype, reshape)\n",
        "        \n",
        "        self.full_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(self.params['NUM_EXAMPLES'])\n",
        "     \n",
        "        print(\"PARAMS: reshape=\", reshape, \" from \", raw_features.shape, \" dtype=\", dtype, \" batch size=\", \n",
        "        params['BATCH_SIZE'], \" train size=\", params['TRAIN_SIZE'])\n",
        "\n",
        "    \n",
        "    def preprocess(self, features, labels, dtype, reshape):\n",
        "        assert len(features) == len(labels)\n",
        "        if not isinstance(features, dtype):\n",
        "            features = dtype(features)\n",
        "        if reshape is not False:\n",
        "            features = features.reshape(reshape)\n",
        "        labels = np.array(Pool(self.params['NUM_CORES']).map(self.encode_to_one_hot, labels))\n",
        "        return np.asarray(features), np.asarray(labels)\n",
        "    \n",
        "    \n",
        "    def get_dataset(self, mode):\n",
        "        dataset, buff, batch_size = self.split_dataset(mode)\n",
        "        dataset = dataset.cache().shuffle(buff).repeat().batch(batch_size).prefetch(1)\n",
        "        return\n",
        "\n",
        "      \n",
        "    def split_dataset(self, mode):\n",
        "        if mode == 'training':\n",
        "            return ()\n",
        "        elif mode == 'validation':\n",
        "            return (self.full_dataset.skip(self.params['TRAIN_SIZE']), self.params['VALID_SIZE'],\n",
        "                    self.params['VALID_SIZE'])\n",
        "        else:\n",
        "            raise ArgumentError\n",
        "    \n",
        "    def encode_to_one_hot(self, label):\n",
        "        one_hot_encoding = np.zeros(self.params['NUM_CLASSES'])\n",
        "        one_hot_encoding[label] = 1\n",
        "        return one_hot_encoding    \n",
        "    \n",
        "    def run(self, iterations, profile):\n",
        "        train_dataset = self.full_dataset.take(self.params['TRAIN_SIZE']).cache().shuffle(\n",
        "            self.params['TRAIN_SIZE']).repeat().batch(self.params['BATCH_SIZE']).prefetch(1)\n",
        "        \n",
        "        valid_dataset = self.full_dataset.skip(self.params['TRAIN_SIZE']).cache().shuffle(\n",
        "            self.params['VALID_SIZE']).repeat().batch(self.params['VALID_SIZE']).prefetch(1)\n",
        "        \n",
        "        # create general iterator\n",
        "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                               train_dataset.output_shapes)\n",
        "        next_batch = iterator.get_next()\n",
        "        \n",
        "        # make datasets that we can initialize separately, but using the same structure via the common iterator\n",
        "        training_init_op = iterator.make_initializer(train_dataset)\n",
        "        validation_init_op = iterator.make_initializer(valid_dataset)      \n",
        "        \n",
        "        model_function = self.model.get_model_function()\n",
        "        logit, label = model_function(next_batch[0]), next_batch[1]\n",
        "        loss = self.model.get_loss(logit, label)\n",
        "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        accuracy = self.model.get_accuracy(logit, label)\n",
        "        \n",
        "        if profile is not False:\n",
        "            # graph_writer = tf.summary.FileWriter(\"pipeline\", sess.graph)\n",
        "            run_metadata = tf.RunMetadata()\n",
        "            options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "        else:\n",
        "            run_metadata = None\n",
        "            options = None\n",
        "        \n",
        "        # Create a saver for writing training checkpoints.\n",
        "        saver = tf.train.Saver()\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            sess.run([tf.global_variables_initializer()], options=options, run_metadata=run_metadata)\n",
        "            avg_acc = 0\n",
        "            print(\"START BENCHMARK\")\n",
        "            start = time.time()\n",
        "            for i in range(iterations):\n",
        "                sess.run(training_init_op)\n",
        "                loss_, _, acc = sess.run([loss, optimizer, accuracy], options=options, run_metadata=run_metadata)\n",
        "                sess.run(validation_init_op)\n",
        "                val_acc = sess.run(accuracy, options=options, run_metadata=run_metadata)\n",
        "#                 avg_acc += acc\n",
        "                if i % int(iterations/10) == 0:\n",
        "                    print(\"acc, val_acc\", acc*100, val_acc*100)\n",
        "#                     print(\"Epoch: {}, loss: {:.3f}, {} accuracy: {:.2f}%\".format(i, loss_, acc * 100))\n",
        "#                     print(\"validation accuracy: {:.2f}%\".format(val_acc * 100))\n",
        "            \n",
        "            print(\"Average {} accuracy over {} iterations is {:.2f}%\".format(iterations, (avg_acc / iterations) * 100))\n",
        "            end = time.time()\n",
        "            time_per_run = end - start\n",
        "            avg_acc = avg_acc/iterations\n",
        "            \n",
        "            print(\"END BENCHMARK. TIME: \", time_per_run)\n",
        "        \n",
        "            self.generate_trace(run_metadata, profile)\n",
        "            return time_per_run, avg_acc\n",
        "    \n",
        "    def generate_trace(self, run_metadata, profile):\n",
        "        if profile is False:\n",
        "            return\n",
        "        else:\n",
        "            # Create the Timeline object, and write it to a json file\n",
        "            fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n",
        "            chrome_trace = fetched_timeline.generate_chrome_trace_format()\n",
        "            with open(profile, 'w') as f:\n",
        "                f.write(chrome_trace)\n",
        "            return\n",
        "\n",
        "    def should_reshape(self, rank):\n",
        "        if rank not in self.model.get_required_dims():\n",
        "            if rank == 4: # e.g. bfimage, FullyConnected\n",
        "                return (-1, self.params['HEIGHT']*self.params['WIDTH']*self.params['CHANNELS'])\n",
        "            elif rank == 2: # e.g. mnist, LowResFrameClassifier\n",
        "                return (-1, self.params['HEIGHT'], self.params['WIDTH'], self.params['CHANNELS'])\n",
        "            else:\n",
        "                ValueError(\"Invalid data shape\")\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def load_params(config_path, param={}):\n",
        "    '''\n",
        "    Load parameters from file.\n",
        "    '''\n",
        "    params = {}\n",
        "    if not os.path.isfile(config_path):\n",
        "        raise KeyError\n",
        "    else:\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            params.update(toml.load(f))\n",
        "        return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_teDADxz4fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "587e684f-e867-4b50-8b19-2ebf34595011"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from mnist import MNIST\n",
        "import math\n",
        "\n",
        "def main():\n",
        "    # GPU utilization\n",
        "    NUM_RUNS = 1\n",
        "    PATH_TO_CHROME_TRACES = \"/project/datasets-API/benchmark-package/chrome-traces/test.json\"\n",
        "    \n",
        "    benchmark = Benchmark('FullyConnected')\n",
        "    tot_acc, tot_time = 0, 0\n",
        "    for i in range(NUM_RUNS):\n",
        "        tot_time += benchmark.run(1000, profile=False)[0]\n",
        "        # tot_acc += benchmark.run('validation', 1000, profile=False)[1]\n",
        "    print(\"RESULTS: Train time=\", tot_time/NUM_RUNS, \", Validation accuracy=\", tot_acc/NUM_RUNS*100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 'FullyConnected' on 'mnist'\n",
            "PARAMS: reshape= False  from  (60000, 784)  dtype= <class 'numpy.float32'>  batch size= 48  train size= 48000\n",
            "START BENCHMARK\n",
            "acc, val_acc 0.125 0.068833336\n",
            "acc, val_acc 0.7291667 0.8269167\n",
            "acc, val_acc 0.9166667 0.8444167\n",
            "acc, val_acc 0.7916667 0.8685833\n",
            "acc, val_acc 0.9166667 0.8699167\n",
            "acc, val_acc 0.8958333 0.87991667\n",
            "acc, val_acc 0.875 0.87175\n",
            "acc, val_acc 0.8541667 0.837\n",
            "acc, val_acc 0.8333333 0.86466664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SAtnOhksz6SE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}