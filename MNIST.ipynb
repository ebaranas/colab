{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebaranas/colab/blob/master/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GpQFUZMLm7Je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, model_name, params):\n",
        "        self.NUM_CLASSES = params['NUM_CLASSES']\n",
        "        SUPPORTED_MODELS= {\"FullyConnected\": (self.FullyConnected, [2]),\n",
        "                    \"LowResFrameClassifier\": (self.LowResFrameClassifier, [4]),\n",
        "                    \"SimpleFullyConnected\":(self.SimpleFullyConnected, [2])\n",
        "        }\n",
        "        self.model_function, self.required_dims = SUPPORTED_MODELS.get(model_name, 'KeyError')\n",
        "        \n",
        "    def get_model_function(self):\n",
        "        return self.model_function\n",
        "    \n",
        "    def get_required_dims(self):\n",
        "        # For now first element of list is default.\n",
        "        return self.required_dims\n",
        "    \n",
        "    def train(self, next_batch):\n",
        "        logit, label = self.model_function(next_batch[0]), next_batch[1]\n",
        "        loss = self.get_loss(logit, label)\n",
        "        optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "        accuracy = self.get_accuracy(logit, label)\n",
        "        return [loss, optimizer, accuracy]\n",
        "    \n",
        "    def get_loss(self, logit, label):\n",
        "        softmax = tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit)\n",
        "        return tf.reduce_sum(softmax)\n",
        "\n",
        "    def get_accuracy(self, logit, label):\n",
        "        prediction = tf.argmax(logit, 1)\n",
        "        equality = tf.equal(prediction, tf.argmax(label, 1))\n",
        "        return tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        \n",
        "    def FullyConnected(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        fc1 = tf.layers.dense(feature, 50)\n",
        "        fc2 = tf.layers.dense(fc1, 50)\n",
        "        fc2 = tf.layers.dropout(fc2)\n",
        "        flat = tf.layers.flatten(fc2) # added flatten layer 08/10/2018 to correct shape\n",
        "        fc3 = tf.layers.dense(flat, self.NUM_CLASSES)\n",
        "        return fc3\n",
        "        \n",
        "    def SimpleFullyConnected(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        fc1 = tf.layers.dense(feature, 30)\n",
        "        final = tf.layers.dense(fc1, self.NUM_CLASSES)\n",
        "        return final\n",
        "    \n",
        "    def LowResFrameClassifier(self, feature):\n",
        "        #bn = tf.layers.batch_normalization(feature)\n",
        "        conv1 = tf.layers.conv2d(feature, 32, (3, 3), activation=\"relu\")\n",
        "        conv2 = tf.layers.conv2d(conv1, 32, (3, 3), activation=\"relu\")\n",
        "        maxpool2d_a = tf.layers.max_pooling2d(conv2, pool_size=(2, 2), strides=(1, 1))\n",
        "        dropout_a = tf.layers.dropout(maxpool2d_a, rate=0.25)\n",
        "        \n",
        "        conv3 = tf.layers.conv2d(dropout_a, 64, (3, 3), activation=\"relu\")\n",
        "        conv4 = tf.layers.conv2d(conv3, 64, (3, 3), activation=\"relu\")\n",
        "        maxpool2d_b = tf.layers.max_pooling2d(conv3, pool_size=(2, 2), strides=(1, 1))\n",
        "        dropout_b = tf.layers.dropout(maxpool2d_b, rate=0.25)\n",
        "        \n",
        "        flat = tf.layers.flatten(dropout_b)\n",
        "        dense = tf.layers.dense(flat, 256, activation=\"relu\")\n",
        "        dropout_c = tf.layers.dropout(dense, rate=0.25)\n",
        "        \n",
        "        final = tf.layers.dense(dropout_c, self.NUM_CLASSES, activation=\"softmax\")\n",
        "        return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5jgPUNpf3Ckv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from multiprocessing import Pool\n",
        "import tensorflow as tf\n",
        "import main as m\n",
        "\n",
        "# Pipeline strategy interface\n",
        "class PipelineStrategy(object):\n",
        "    ''' Simulated abstract base class for pipeline strategy '''\n",
        "\n",
        "    def preprocess(self, features, labels, reshape):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def split_dataset(self, mode):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def get_next_batch(self, mode):\n",
        "        raise NotImplementedError  \n",
        "    \n",
        "    def feed_dict(self, next_batch, mode):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def initializers(self, iterator):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def transform(dataset):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def augment(feature, label):\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def shuffle(dataset):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Pipeline strategy 1\n",
        "class FeedDictPipeline(PipelineStrategy):\n",
        "    def __init__(self, features, labels, params):\n",
        "        self.EPOCHS_COMPLETED = 0\n",
        "        self.INDEX_IN_EPOCH = 0\n",
        "        print(params['NUM_EXAMPLES'])\n",
        "        self.features, self.labels = self.shuffle(features, labels, params['NUM_EXAMPLES'])\n",
        "        self.params = params\n",
        "\n",
        "    def get_next_batch(self, mode=None):\n",
        "        '''Placeholders only'''\n",
        "        replace_index = lambda tuple_: (None,) + tuple_[1:] # to turn NHWC to ?HWC\n",
        "        return tuple((tf.placeholder(self.features.dtype, replace_index(self.features.shape)),\n",
        "        tf.placeholder(self.labels.dtype, replace_index(self.labels.shape))\n",
        "        ))\n",
        "    \n",
        "    def feed_dict(self, next_batch, mode):\n",
        "        '''Returns a dictionary of next batch'''\n",
        "        features, labels, buff = self.split_dataset(mode)\n",
        "        start = self.INDEX_IN_EPOCH\n",
        "        self.INDEX_IN_EPOCH += self.params['BATCH_SIZE']\n",
        "        if self.INDEX_IN_EPOCH > self.params['TRAIN_SIZE']: # fix to include validation\n",
        "            self.EPOCHS_COMPLETED += 1\n",
        "            self.shuffle(features, labels, buff)\n",
        "            start = 0\n",
        "            self.INDEX_IN_EPOCH = self.params['BATCH_SIZE']\n",
        "            assert self.params['BATCH_SIZE'] <= self.params['TRAIN_SIZE']\n",
        "        end = self.INDEX_IN_EPOCH\n",
        "        return {next_batch: (self.features[start:end], self.labels[start:end])}\n",
        "\n",
        "    def split_dataset(self, mode):\n",
        "        if mode == 'training':\n",
        "            return (self.features[:self.params['TRAIN_SIZE']], self.labels[:self.params['TRAIN_SIZE']], \n",
        "            self.params['TRAIN_SIZE'])\n",
        "        elif mode == 'validation':\n",
        "            return (self.features[self.params['TRAIN_SIZE']:], self.labels[self.params['TRAIN_SIZE']:],\n",
        "            self.params['NUM_EXAMPLES'] - self.params['TRAIN_SIZE'])\n",
        "        else:\n",
        "            raise ArgumentError\n",
        "    \n",
        "    def initializers(iterator):\n",
        "        return tf.global_variables_initializer()\n",
        "    \n",
        "    def transform(self, dataset):\n",
        "        p = Pool(self.params['NUM_CORES'])\n",
        "        return p.map(self.augment, zip(*dataset))\n",
        "\n",
        "    def augment(self, example):\n",
        "        return example\n",
        "        \n",
        "    def shuffle(self, features, labels, buff):\n",
        "        perm = np.arange(buff)\n",
        "        np.random.shuffle(perm)\n",
        "        return features[perm], labels[perm]\n",
        "\n",
        "# pipeline strategy 2\n",
        "class DataAPIPipeline(PipelineStrategy):\n",
        "    def __init__(self, features, labels, params):\n",
        "        self.params = params\n",
        "        self.full_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(self.params['NUM_EXAMPLES'])\n",
        "        self.iterator = None\n",
        "    \n",
        "    def get_next_batch(self, mode):\n",
        "        dataset, buff = self.split_dataset(mode)\n",
        "        dataset = dataset.cache().shuffle(buff).repeat().batch(self.params['BATCH_SIZE']).prefetch(1)\n",
        "        # dataset = dataset.shuffle(buff).repeat().batch(self.params['BATCH_SIZE'])\n",
        "        self.iterator = dataset.make_initializable_iterator()\n",
        "        return self.iterator.get_next()\n",
        "    \n",
        "    def feed_dict(self, next_batch=None, mode=None):\n",
        "        return None \n",
        "\n",
        "    def split_dataset(self, mode):\n",
        "        if mode == 'training':\n",
        "            return (self.full_dataset.take(self.params['TRAIN_SIZE']), self.params['TRAIN_SIZE'])\n",
        "        elif mode == 'validation':\n",
        "            return (self.full_dataset.skip(self.params['TRAIN_SIZE']), \n",
        "            self.params['NUM_EXAMPLES'] - self.params['TRAIN_SIZE'])\n",
        "        else:\n",
        "            raise ArgumentError\n",
        "\n",
        "    def initializers(self):\n",
        "        return [tf.global_variables_initializer(), self.iterator.initializer] \n",
        "\n",
        "    def transform(self, dataset):\n",
        "        return dataset.map(lambda feature, label: self.augment(feature, label))\n",
        "\n",
        "    def augment(self, feature, label):\n",
        "        #feature = tf.image.random_hue(feature, 0.5)\n",
        "        pass\n",
        "   \n",
        "    def shuffle(self, dataset):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DRpyktZW3J8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mnist import MNIST\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def read_raw_mnist(data_path):\n",
        "    mndata = MNIST(data_path)\n",
        "    images, labels = mndata.load_training()\n",
        "    return np.asarray(images), np.asarray(labels)\n",
        "\n",
        "def read_raw_bfimage(data_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label_index, label in sorted(enumerate(os.listdir(data_path))):\n",
        "        filename = data_path + label\n",
        "        label = label.replace(\".npy\", \"\")\n",
        "        for image in np.load(filename): # will load one npy file which may contain many examples\n",
        "            images.append(image)\n",
        "            labels.append(label_index)\n",
        "    return np.asarray(images), np.asarray(labels)\n",
        "\n",
        "SUPPORTED_DATA = {\"mnist\": read_raw_mnist, \"bfimage\": read_raw_bfimage}\n",
        "\n",
        "def read_raw(data_name, data_path):\n",
        "    read = (SUPPORTED_DATA.get(data_name, 'KeyError'))\n",
        "    return read(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTejeNtVpFNE",
        "colab_type": "code",
        "outputId": "957c77b9-9e7a-4f1b-8194-479ebe19fc99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install python-mnist\n",
        "from mnist import MNIST"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-mnist in /usr/local/lib/python3.6/dist-packages (0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_LeMzm3Ql7G1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from models import Model\n",
        "from pipelines import DataAPIPipeline\n",
        "from pipelines import FeedDictPipeline\n",
        "import toml\n",
        "from tensorflow.python.client import timeline\n",
        "from multiprocessing import Pool\n",
        "import data as D\n",
        "\n",
        "params = {'DATA_NAME' = 'mnist', 'DATA_PATH' = './', 'DTYPE' = 'np.float32', 'HEIGHT' = 28, \n",
        "          'WIDTH' = 28, 'CHANNELS' = 1, 'NUM_CLASSES' = 10, 'BATCH_SIZE' = 48, 'TRAIN_RATIO' = 0.8, 'NUM_CORES' = 4}\n",
        "\n",
        "class Benchmark(object):\n",
        "    def __init__(self, pipeline, model_name, config_path):\n",
        "        params = load_params(config_path)\n",
        "        print(\"Training '{model}' on '{data}' using '{pipeline}'\".format(model=model_name,\n",
        "            data=params['DATA_NAME'], pipeline=pipeline))\n",
        "        raw_features, raw_labels = D.read_raw(params['DATA_NAME'], params['DATA_PATH'])\n",
        "        \n",
        "        params.update({'NUM_EXAMPLES': len(raw_features)})\n",
        "        params.update({'TRAIN_SIZE': int(params['TRAIN_RATIO']*params['NUM_EXAMPLES'])})\n",
        "        self.params = params\n",
        "        self.model = Model(model_name, params)\n",
        "        SUPPORTED_PIPELINES = {\"feed_dict\": FeedDictPipeline, \"data_API\": DataAPIPipeline}\n",
        "        PipelineClass = SUPPORTED_PIPELINES.get(pipeline, 'KeyError')\n",
        "        \n",
        "        reshape = self.should_reshape(rank=len(raw_features.shape))\n",
        "        dtype = eval(params['DTYPE'])\n",
        "        features, labels = self.preprocess(raw_features, raw_labels, dtype, reshape)\n",
        "        \n",
        "        self.pipeline = PipelineClass(features, labels, params)\n",
        "        print(\"PARAMS: reshape=\", reshape, \" from \", raw_features.shape, \" dtype=\", dtype, \" batch size=\", \n",
        "        params['BATCH_SIZE'], \" train size=\", params['TRAIN_SIZE'])\n",
        "\n",
        "    \n",
        "    def preprocess(self, features, labels, dtype, reshape):\n",
        "        assert len(features) == len(labels)\n",
        "        if not isinstance(features, dtype):\n",
        "            features = dtype(features)\n",
        "        if reshape is not False:\n",
        "            features = features.reshape(reshape)\n",
        "        labels = np.array(Pool(self.params['NUM_CORES']).map(self.encode_to_one_hot, labels))\n",
        "        return np.asarray(features), np.asarray(labels)\n",
        "    \n",
        "    def encode_to_one_hot(self, label):\n",
        "        one_hot_encoding = np.zeros(self.params['NUM_CLASSES'])\n",
        "        one_hot_encoding[label] = 1\n",
        "        return one_hot_encoding    \n",
        "    \n",
        "    def run(self, mode, iterations, profile):\n",
        "        next_batch = self.pipeline.get_next_batch(mode)\n",
        "        fetches = self.model.train(next_batch)\n",
        "        if profile is not False:\n",
        "            # graph_writer = tf.summary.FileWriter(\"pipeline\", sess.graph)\n",
        "            run_metadata = tf.RunMetadata()\n",
        "            options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "        else:\n",
        "            run_metadata = None\n",
        "            options = None\n",
        "        \n",
        "        with tf.Session() as sess:\n",
        "            sess.run(self.pipeline.initializers(), options=options, run_metadata=run_metadata)\n",
        "            avg_acc = 0\n",
        "            print(\"START BENCHMARK\")\n",
        "            start = time.time()\n",
        "            for i in range(iterations):\n",
        "                loss, optimizer, accuracy = sess.run(fetches,\n",
        "                feed_dict=self.pipeline.feed_dict(next_batch, mode),\n",
        "                options=options, run_metadata=run_metadata)\n",
        "                avg_acc += accuracy\n",
        "                if i % int(iterations/10) == 0:\n",
        "                    print(\"Epoch: {}, loss: {:.3f}, {} accuracy: {:.2f}%\".format(i, loss, mode, accuracy * 100))\n",
        "            \n",
        "            print(\"Average {} accuracy over {} iterations is {:.2f}%\".format(mode, iterations, (avg_acc / iterations) * 100))\n",
        "            end = time.time()\n",
        "            time_per_run = end - start\n",
        "            avg_acc = avg_acc/iterations\n",
        "            \n",
        "            print(\"END BENCHMARK. TIME: \", time_per_run)\n",
        "        \n",
        "            self.generate_trace(run_metadata, profile)\n",
        "            return time_per_run, avg_acc\n",
        "    \n",
        "    def generate_trace(self, run_metadata, profile):\n",
        "        if profile is False:\n",
        "            return\n",
        "        else:\n",
        "            # Create the Timeline object, and write it to a json file\n",
        "            fetched_timeline = timeline.Timeline(run_metadata.step_stats)\n",
        "            chrome_trace = fetched_timeline.generate_chrome_trace_format()\n",
        "            with open(profile, 'w') as f:\n",
        "                f.write(chrome_trace)\n",
        "            return\n",
        "\n",
        "    def should_reshape(self, rank):\n",
        "        if rank not in self.model.get_required_dims():\n",
        "            if rank == 4: # e.g. bfimage, FullyConnected\n",
        "                return (-1, self.params['HEIGHT']*self.params['WIDTH']*self.params['CHANNELS'])\n",
        "            elif rank == 2: # e.g. mnist, LowResFrameClassifier\n",
        "                return (-1, self.params['HEIGHT'], self.params['WIDTH'], self.params['CHANNELS'])\n",
        "            else:\n",
        "                ValueError(\"Invalid data shape\")\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def load_params(config_path, param={}):\n",
        "    '''\n",
        "    Load parameters from file.\n",
        "    '''\n",
        "    params = {}\n",
        "    if not os.path.isfile(config_path):\n",
        "        raise KeyError\n",
        "    else:\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            params.update(toml.load(f))\n",
        "        return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKuM8Dtsn3vt",
        "colab_type": "code",
        "outputId": "b9187595-8477-4d31-fbb7-623fdd769cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from mnist import MNIST\n",
        "import math\n",
        "\n",
        "def main():\n",
        "    # GPU utilization\n",
        "    NUM_RUNS = 1\n",
        "    PATH_TO_CHROME_TRACES = \"/project/datasets-API/benchmark-package/chrome-traces/test.json\"\n",
        "    \n",
        "    benchmark = Benchmark('data_API', 'FullyConnected')\n",
        "    tot_acc, tot_time = 0, 0\n",
        "    for i in range(NUM_RUNS):\n",
        "        tot_time += benchmark.run('training', 500, profile=False)[0]\n",
        "        # tot_acc += benchmark.run('validation', 1000, profile=False)[1]\n",
        "    print(\"RESULTS: Train time=\", tot_time/NUM_RUNS, \", Validation accuracy=\", tot_acc/NUM_RUNS*100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 'FullyConnected' on 'mnist'\n",
            "PARAMS: reshape= (-1, 28, 28, 1)  from  (60000, 784)  dtype= <class 'numpy.float32'>  batch size= 64  train size= 48000\n",
            "START BENCHMARK\n",
            "Epoch: 0, loss: 2210.548, training accuracy: 4.69%\n",
            "Epoch: 60, loss: 2425.855, training accuracy: 9.38%\n",
            "Epoch: 120, loss: 2452.834, training accuracy: 3.12%\n",
            "Epoch: 180, loss: 2334.035, training accuracy: 9.38%\n",
            "Epoch: 240, loss: 2543.234, training accuracy: 4.69%\n",
            "Epoch: 300, loss: 2299.873, training accuracy: 6.25%\n",
            "Epoch: 360, loss: 2139.520, training accuracy: 10.94%\n",
            "Epoch: 420, loss: 2525.710, training accuracy: 4.69%\n",
            "Epoch: 480, loss: 2215.819, training accuracy: 4.69%\n",
            "Epoch: 540, loss: 2343.555, training accuracy: 7.81%\n",
            "END BENCHMARK. TIME:  97.03578329086304\n",
            "RESULTS: Train time= 97.03578329086304 , Validation accuracy= 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mcnxA6Gkn8wy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}